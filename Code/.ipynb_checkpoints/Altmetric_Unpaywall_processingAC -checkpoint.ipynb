{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/yls1/.local/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /home/yls1/.local/lib/python3.9/site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy in /home/yls1/.local/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: seaborn in /home/yls1/.local/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /home/yls1/.local/lib/python3.9/site-packages (1.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3.9/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yls1/.local/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yls1/.local/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/yls1/.local/lib/python3.9/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/yls1/.local/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/yls1/.local/lib/python3.9/site-packages (from matplotlib) (6.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/yls1/.local/lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yls1/.local/lib/python3.9/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/yls1/.local/lib/python3.9/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/yls1/.local/lib/python3.9/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/yls1/.local/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/yls1/.local/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/yls1/.local/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/yls1/.local/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /home/yls1/.local/lib/python3.9/site-packages\n",
      "sysconfig: /home/yls1/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib numpy seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT package \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Scottish Funding Council recognises and funds 19 universities in Scotland, which we get 18 universities and divided them into four classifications. There are respectly:Ancient,Modern,Chartered,Specialist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_group(x) :\n",
    "    grouping_type = \"\"\n",
    "    g1 = [\"Glasgow School of Art\",\"Royal Conservatoire of Scotland\",\"Scotland's Rural College\"]\n",
    "    g2 = [\"University of Edinburgh\",\"University of Glasgow\",\"University of St Andrews\",\"University of Aberdeen\"]\n",
    "    g3 = [\"University of Dundee\",\"Heriot-Watt University\",\"University of Strathclyde\",\"University of Stirling\"]\n",
    "    g4 = [\"Abertay University\",\"Glasgow Caledonian University\",\"Edinburgh Napier University\",\"Queen Margaret University\",\n",
    "          \"University of the Highlands and Islands\",\"University of the West of Scotland\",\"Robert Gordon University\"]\n",
    "    if x in g1 :\n",
    "        grouping_type = \"Specialist\" ;\n",
    "    elif x in g2 :\n",
    "        grouping_type = \"Ancient\" ;\n",
    "    elif x in g3 :\n",
    "        grouping_type = \"Chartered\" ;\n",
    "    elif x in g4 :\n",
    "        grouping_type = \"Modern\" ;\n",
    "    else :\n",
    "        grouping_type = f\"No Group_{u}\"\n",
    "    return grouping_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulid a function to check the downloaded data from Altmetric :<br>\n",
    "> Altmetric Data :download form(www.altmetric.com) by each university on 30/June/2024\n",
    "  Data description : ...\n",
    "\n",
    "\n",
    "following the step by:\n",
    "- read the downloaded CSV file by university \n",
    "- drop the irrelative data colnumn \n",
    "- check data format and empty data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Alt_data_check_clean(uni) : \n",
    "    # Read in CSV. file (Altmetric Data)\n",
    "    data = pd.read_csv(f\"../Data/Alt/Altmetric_{uni}.csv\", encoding = \"utf-8\")\n",
    "    print(f\"Read the {uni} Alt local data file!\")\n",
    "    \n",
    "    data_extra = data.copy(deep=True)\n",
    "    # data_extra = data_extra.dropna(axis=1, how='all')\n",
    "    data_extra =data_extra.drop(columns =['Outlet or Author','Country','Mention Type','External Mention ID'],axis=1)\n",
    "    print(\"trim the column\")\n",
    "    \n",
    "    print(\"---Unique Numbers---\")\n",
    "    print(f\"Unique DOI counts :{data_extra['DOI'].nunique()}\")\n",
    "    print(f\"Unique Wiki Title counts :{data_extra['Mention Title'].nunique()}\")\n",
    "    print(f\"Total data counts :{len(data_extra)}\")\n",
    "    print(\" \")\n",
    "    \n",
    "    #Check DOI |ArXivID wether miss data\n",
    "    #DOI will use for looking for the OA status by Unpaywall, if non-DOI,it will be unable find OA status.\n",
    "    print(\"---Check DOI ---\")\n",
    "    DOI_NA = data_extra[data_extra['DOI'].isna()==True ]\n",
    "    DOI_Count = len(data_extra) - len(DOI_NA)\n",
    "    \n",
    "    #Check PubMedID exist when DOI is empty  \n",
    "    #While DOI is empty, it the reference source may is book(with PubMed ID),rather DOI\n",
    "    PubMedID_exist =DOI_NA[DOI_NA[\"PubMed ID\"].isna()==False]\n",
    "    PubMedID_na =DOI_NA[DOI_NA[\"PubMed ID\"].isna()==True]\n",
    "    #Check PubMedID |ArXivID wether miss data\n",
    "    # ArXivID_exist =DOI_NA[DOI_NA[\"arXiv ID\"].isna()==False]\n",
    "    # ArXivID_na =DOI_NA[DOI_NA[\"arXiv ID\"].isna()==True]\n",
    "    \n",
    "    print(f\"There are {len(DOI_NA)} empty DOI\")\n",
    "    print(f\"In those empty DOI, there are {len(PubMedID_exist)} with PubMedID \")\n",
    "    print(f\"There are {len(PubMedID_na)} withOUT PubMedID \")\n",
    "    if len(PubMedID_na) > 0:\n",
    "        print(PubMedID_na )\n",
    "    \n",
    "    print(f\"Total : {len(data_extra)} data\")\n",
    "    print(\" \")\n",
    "    DOI_NA_table = pd.DataFrame(np.array([[DOI_Count, len(PubMedID_exist)], [len(DOI_NA), len(PubMedID_na)]]),\n",
    "                       columns=['DOI', 'PubMedID'],index=[\"counts\",\"empty\"])\n",
    "    print(\"Print DOIis empty _table\")\n",
    "    print(DOI_NA_table)\n",
    "\n",
    "    #Check Affiliations wether miss data\n",
    "    print(\" \")\n",
    "    print(\"---Checking Affiliations--- \")\n",
    "    Affil_without_uni = data_extra[~data_extra['Affiliations (GRID)'].str.contains(uni, na=False)]\n",
    "    print(f\"{len(Affil_without_uni)} data , not Uni included in Affiliations \")\n",
    "    print(f\" List the data not include {uni} \")\n",
    "    print(Affil_without_uni)\n",
    "    \n",
    "    \n",
    "    #check Mention Date >funder year\n",
    "    date = data_extra[\"Mention Date\"].str.split(\" \",expand=True) \n",
    "    year = date[0].str.split(\"-\",expand=True)[0]\n",
    "    #print(f\"Ealiest mention year : {min(year)}\")\n",
    "\n",
    "    #Check Mention Date _ date, format \n",
    "    data_extra[['Mention_date','Mention_time']] = data_extra[\"Mention Date\"].str.split(\" \",expand=True)  \n",
    "    try:\n",
    "        # This will raise an error if any date is not in the correct format\n",
    "        data_extra['Mention_date'] = pd.to_datetime(data_extra['Mention_date'], format='%Y-%m-%d', errors='raise')\n",
    "        print(\"All Mention dates are in the correct format.\")\n",
    "    except ValueError:\n",
    "        print(\"Some dates are not in the correct format.\",data_extra['Mention_date'])\n",
    "            \n",
    "    data_extra =data_extra.drop(columns =['Mention Date'],axis=1)\n",
    "    \n",
    "    # Publication Date\n",
    "    try:\n",
    "        # This will raise an error if any date is not in the correct format\n",
    "        data_extra['Publication Date'] = pd.to_datetime(data_extra['Publication Date'], format='%Y-%m-%d', errors='raise')\n",
    "        print(\"All Publication dates are in the correct format.\")\n",
    "    except ValueError:\n",
    "        print(\"Some dates are not in the correct format.\",data_extra['Publication Date'])\n",
    "    #Mention URL\n",
    "    print(\"Any empty URL\")\n",
    "    print(data_extra[data_extra['Mention URL'].isna()== True])\n",
    "\n",
    "    #Wiki_Language code from url  \n",
    "    lan = data_extra[\"Mention URL\"]\n",
    "    lan =lan.str.split(\"//\",expand=True)[1]\n",
    "    Wiki_Lang_code = lan.str.split(\".\",expand=True)[0]\n",
    "    data_extra['Wiki_Language'] = Wiki_Lang_code \n",
    "    print(f\"Top 10 using Language\") #or Top 15\n",
    "    print(data_extra['Wiki_Language'].value_counts()[:10]) \n",
    "\n",
    "    # Altmetric Attention Score\n",
    "    # # print(data_extra['Altmetric Attention Score'].isna().unique())\n",
    "    # try:\n",
    "    #     # This will raise an error if any value is not an integer\n",
    "    #     data_extra['Altmetric Attention Score'] = pd.to_numeric(data_extra['Altmetric Attention Score'], downcast='integer', errors='raise')\n",
    "    #     print(\"All values in the column are integers.\")\n",
    "    # except ValueError:\n",
    "    #     print(\"Some values in the column are not integers.\")\n",
    "    #Subjects (FoR) _spilt grid and subject or not))\n",
    "\n",
    "    #Drop empty DOI\n",
    "    if len(DOI_NA) > 0:\n",
    "        data_cleaned =  data_extra.dropna(subset=['DOI'])\n",
    "        data_cleaned = data_cleaned[data_cleaned['DOI'].str.strip()!='']\n",
    "        data_cleaned.reset_index(drop=True,inplace=True)\n",
    "    else :\n",
    "        data_cleaned = data_extra\n",
    "\n",
    "    print(\"Last Cleaned Data\")\n",
    "    # print(data_cleaned)\n",
    "    return data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulid a function to gather data using DOI to get API response from unpaywall\n",
    "API :(https://api.unpaywall.org/v2/{['DOI']}) <br>\n",
    "re format and orgaine new data from Unpaywall contact with Altmetric data, as compeleted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unpaywalldata(uni,data_cl):\n",
    "\n",
    "    #add info\n",
    "    Uni, Grouping = [],[]\n",
    "    group_type = uni_group(uni)\n",
    "    #Unpaywall data\n",
    "    Is_OA,OA_Status = [], []\n",
    "    Genre ,title, OA_Locations, License_bestOA = [], [], [], []\n",
    "\n",
    "    #API _all DOI unpaywall data \n",
    "    for j in range(len(data_cl)):\n",
    "        Uni.append(uni)\n",
    "        Grouping.append(group_type)\n",
    "        response_API = requests.get(f\"https://api.unpaywall.org/v2/{data_cl['DOI'][j]}?email=unpaywall_01@example.com\")\n",
    "        #print(response_API.status_code)\n",
    "        #Check API status and get individual data \n",
    "        if response_API.status_code ==200 :\n",
    "            Unpaywall_data= response_API.text\n",
    "            parse_json = json.loads(Unpaywall_data)\n",
    "            active_case = parse_json\n",
    "            Is_OA.append(active_case['is_oa'])\n",
    "            OA_Status.append(active_case['oa_status'])\n",
    "            Genre.append(active_case['genre'])\n",
    "            title.append(active_case['title'])\n",
    "            OA_Locations.append(active_case['oa_locations'])\n",
    "            if active_case['is_oa'] is True :\n",
    "                License_bestOA.append(active_case['best_oa_location']['license'])\n",
    "            else :\n",
    "                License_bestOA.append('Not OA')\n",
    "        else :\n",
    "            Is_OA.append('notFound')\n",
    "            OA_Status.append(response_API.status_code)\n",
    "            Genre.append('notFound')\n",
    "            title.append('notFound')\n",
    "            OA_Locations.append('notFound')\n",
    "            License_bestOA.append('notFound')\n",
    "           \n",
    "            \n",
    "    #Keep key data\n",
    "    #clean and turn to dataframe\n",
    "    Is_OA = pd.DataFrame(Is_OA, columns = ['OA_status'])\n",
    "    Uni = pd.DataFrame(Uni , columns = ['University'])\n",
    "    Grouping = pd.DataFrame(Grouping, columns = ['Group'])\n",
    "    OA_Status = pd.DataFrame(OA_Status , columns = ['OA_Type'])\n",
    "    #---more information\n",
    "    Genre = pd.DataFrame(Genre , columns = ['Genre'])\n",
    "    title = pd.DataFrame(title , columns = ['Title'])\n",
    "    OA_Locations = pd.Series(OA_Locations)\n",
    "    OA_Locations = pd.DataFrame(OA_Locations , columns = ['OA_Locations'])\n",
    "    License_bestOA = pd.DataFrame(License_bestOA , columns = ['License_bestOA'])\n",
    "    \n",
    "    \n",
    "    OA_data= pd.concat([Is_OA,OA_Status,Genre,title,OA_Locations,License_bestOA,Uni,Grouping],axis = 1)\n",
    "    #print(OA_data)\n",
    "    New_data = pd.concat([data_cl,OA_data],axis = 1)\n",
    "    # print(New_data)\n",
    "    # # New_data.to_csv(f\"{x}_AltOA.csv\")\n",
    "    # New_data.to_csv(f\"../Data/Unpaywall/New/n_{uni}_AltOA.csv\",index=False)\n",
    "    print(\"Finish dowonload\")\n",
    "    \n",
    "    return  New_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check unpaywall new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulid a function to Check Altmetric+ Unpaywall data\n",
    "\n",
    "- Check OA_status and OA_Type\n",
    "- deal with dupilcation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpaywall_df_check(ndf) :\n",
    "    # print(ndf.info())\n",
    "    print(\"Check OA_status\")\n",
    "    print(\"Not found OA_status\")\n",
    "    if len(ndf[ndf['OA_status'].isna() == False]) ==0 :\n",
    "        print(\"There is not empty OA_status\")\n",
    "    else  :\n",
    "        print(ndf[ndf['OA_status'].isna()== True])\n",
    "    \n",
    "    #Check 'OA_status' is either True or False\n",
    "    print(ndf['OA_status'].unique())\n",
    "    desired_bool = [False, True]\n",
    "    error_status = ndf[~ndf[\"OA_status\"].isin(desired_bool)]\n",
    "    print(f\"There are TOTAL : {error_status.shape[0]} Data  cannot find through Unpaywall.\")\n",
    "    print(f\"There are {ndf[ndf['OA_status']=='notFound'].shape[0]} Data  cannot find through Unpaywall.\")\n",
    "    #List data entries which \"notFound\" for further check\n",
    "    print(ndf[ndf['OA_status']==\"notFound\"][['DOI','OA_Type']])\n",
    "    \n",
    "    #Check 'OA_Type' is only those 5 types ['closed','bronze','green','gold','hybrid']\n",
    "    print(ndf['OA_Type'].unique())\n",
    "    desired_Type= ['closed','bronze','green','gold','hybrid']\n",
    "    error_data = ndf[~ndf[\"OA_Type\"].isin(desired_Type)]\n",
    "    print(\" \")\n",
    "    #List data entries which not expected OA Type for further check\n",
    "    #confirm the unexpected OA_status and OA_Type is the same data entry \n",
    "    print(\"error data _list (Doi)\")\n",
    "    print(error_data[[\"DOI\",'OA_status','OA_Type',\"Mention Title\"]])\n",
    "    print(\"------\")\n",
    "    \n",
    "    #1 Cleaned Data (remove empty DOI/error status entries)\n",
    "    cl_data = ndf[ndf['OA_status']!= \"notFound\"]\n",
    "    print(\"-- Cleaned Data count --\")\n",
    "    print(f\"Total catation : {cl_data.shape[0]}\")\n",
    "    print(\"University OA_ Status count\")\n",
    "    print(cl_data['OA_status'].value_counts())\n",
    "    print(\"University OA_ Type count\")\n",
    "    print(cl_data['OA_Type'].value_counts())\n",
    "    print(\"------\")\n",
    "    \n",
    "    #Explore Duplication \n",
    "    full_duplicates = cl_data[cl_data.duplicated(subset=[\"DOI\",\"Mention Title\",\"Mention URL\",'Mention_date','Wiki_Language'],keep=False)]\n",
    "    print(\"About truly duplicates\")\n",
    "    print(f\"There are {full_duplicates['DOI'].nunique()} unique DOI in Duplicate list \")\n",
    "    print(f\"There are {full_duplicates['Mention Title'].nunique()} unique Wiki Title in Duplicate list \")\n",
    "    print(f\"There are Total : {len(full_duplicates)} data in Duplicate list\")\n",
    "    print(\" \")\n",
    "    \n",
    "    \n",
    "    duplicates = cl_data[cl_data.duplicated(subset=[\"DOI\",\"Mention Title\",'Mention_date','Wiki_Language'],keep=False)]\n",
    "    print(\"About potential duplicates\")\n",
    "    print(f\"There are {duplicates['DOI'].nunique()} unique DOI in  potential Duplicate list \")\n",
    "    print(f\"There are {duplicates['Mention Title'].nunique()} unique Wiki Title in potential Duplicate list \")\n",
    "    print(f\"There are Total : {len(duplicates)} data in potential Duplicate list \")\n",
    "    \n",
    "    \n",
    "    #2 De-truly Duplication Data\n",
    "    No_du_data = cl_data.drop_duplicates(subset=[\"DOI\",\"Mention Title\",\"Mention URL\",'Mention_date','Wiki_Language'],keep=\"last\")\n",
    "    No_du_data = No_du_data[No_du_data['OA_status']!= \"notFound\"]\n",
    "    print(f\"De-truly Duplication Total catation : {No_du_data.shape[0]}\")\n",
    "    #3 De-potential Duplicaion Data\n",
    "    No_pdu_data = cl_data.drop_duplicates(subset=[\"DOI\",\"Mention Title\",'Mention_date','Wiki_Language'],keep=\"last\")\n",
    "    No_pdu_data = No_pdu_data[No_pdu_data['OA_status']!= \"notFound\"]\n",
    "    print(f\"De-potential Duplication Total catation : {No_pdu_data.shape[0]}\")\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"Show all data count and percentage\")\n",
    "    def calculate_counts_and_percentages(data):     \n",
    "        oaStatus_counts = data['OA_status'].value_counts()\n",
    "        oaStatus_percentages = data['OA_status'].value_counts(normalize = True)*100\n",
    "        \n",
    "        OpenOA = data[data['OA_status']==True]\n",
    "        OpenOAType_counts = OpenOA ['OA_Type'].value_counts()\n",
    "        OpenOAType_percentages  = OpenOA ['OA_Type'].value_counts(normalize = True)*100\n",
    "    \n",
    "        OA_status_table = pd.DataFrame({\n",
    "           'count':oaStatus_counts ,\n",
    "           'percentage':oaStatus_percentages\n",
    "        })\n",
    "    \n",
    "        OA_type_table = pd.DataFrame({\n",
    "           'count':OpenOAType_counts ,\n",
    "           'percentage':OpenOAType_percentages\n",
    "        })\n",
    "        \n",
    "        data_countandpercent = pd.concat([OA_status_table,OA_type_table] ,keys=['OA_status','OA_Type'])\n",
    "        return data_countandpercent\n",
    "    \n",
    "    #1+2+3    \n",
    "    combined_data = pd.concat([calculate_counts_and_percentages(cl_data),calculate_counts_and_percentages(No_du_data),calculate_counts_and_percentages(No_pdu_data)] ,axis=1)\n",
    "    \n",
    "    print(combined_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check and gather data by each universities in Ancient group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Group_A = [\"University of Aberdeen\",\"University of St Andrews\",\"University of Edinburgh\",\"University of Glasgow\"]\n",
    "for uni in Group_A :\n",
    "    print(f\"!!{uni}_Start\")\n",
    "    data_cleaned  = Alt_data_check_clean(uni) \n",
    "    New_data = Unpaywalldata(uni,data_cleaned)\n",
    "    print(\"+++------------------------\")\n",
    "    print('check unpaywall new data')\n",
    "    # print( New_data.info())\n",
    "    unpaywall_df_check(New_data)\n",
    "    print(f\"{uni}_done\")\n",
    "    print(\"------------------------\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check and gather data by each universities in Chartered group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Group_C = [\"University of Dundee\",\"Heriot-Watt University\",\"University of Strathclyde\",\"University of Stirling\"]\n",
    "for uni in Group_C :\n",
    "    print(f\"!!{uni}_Start\")\n",
    "    data_cleaned  = Alt_data_check_clean(uni) \n",
    "    New_data = Unpaywalldata(uni,data_cleaned)\n",
    "    print(\"+++------------------------\")\n",
    "    print('check unpaywall new data')\n",
    "    # print( New_data.info())\n",
    "    unpaywall_df_check(New_data)\n",
    "    print(f\"{uni}_done\")\n",
    "    print(\"------------------------\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "due to running time, code lenght spereated in 2 sheets by grouping 1 and grouping2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
